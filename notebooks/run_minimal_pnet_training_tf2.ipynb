{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc6e8db",
   "metadata": {
    "name": "06329cad-8ffb-477a-b415-1947059ccaad"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal P-NET Training Script for TensorFlow 2.x\n",
    "Adapted from PyTorch P-NET testing workflow for prostate cancer project.\n",
    "\n",
    "This script demonstrates end-to-end training of a P-NET model using the minimal \n",
    "prostate dataset with TensorFlow 2.x and the project's existing utilities.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75136f51-0fd0-4ab7-babc-41deed318514",
   "metadata": {
    "name": "2741c939-2a18-46d6-b9f0-5d8d17a9fbca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow scikit-learn matplotlib pandas pyyaml jupytext networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a8a24a3",
   "metadata": {
    "name": "5d808747-52dc-4b9e-8e30-261c0ad85844"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6adcf80",
   "metadata": {
    "name": "f1cda08e-8ba4-4184-ae12-cef3b600dbfa"
   },
   "outputs": [],
   "source": [
    "# Ensure the project root is in Python path\n",
    "sys.path.insert(0, '/procedure/pnet_prostate_paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2628434d",
   "metadata": {
    "name": "68381a4e-fcd7-4406-9828-93564eaf5ae9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# TensorFlow 2.x imports\u001b[39;00m\n\u001b[32m      2\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpip install tensorflow\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ModelCheckpoint\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# TensorFlow 2.x imports\n",
    "!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6841657",
   "metadata": {
    "lines_to_next_cell": 1,
    "name": "36e0d3bb-6fe8-4b1e-aa33-8ba4d778e2ff"
   },
   "outputs": [],
   "source": [
    "# Project-specific imports\n",
    "from data.data_access import Data\n",
    "from model.builders.prostate_models import build_pnet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f17679",
   "metadata": {
    "name": "473f18ba-331b-43f2-afb5-aafe298c0962"
   },
   "outputs": [],
   "source": [
    "# Define TF2-compatible F1 metric\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"TensorFlow 2.x compatible F1 score metric.\"\"\"\n",
    "    # Cast y_true to float32 to match y_pred type\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    \n",
    "    # Threshold predictions\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "    \n",
    "    # Calculate true positives, false positives, false negatives\n",
    "    tp = tf.reduce_sum(y_true * y_pred)\n",
    "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "    fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
    "    recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9914518",
   "metadata": {
    "name": "8e14caf3-ae11-4d68-80eb-27db704b44f5"
   },
   "outputs": [],
   "source": [
    "def setup_logging(log_level='INFO'):\n",
    "    \"\"\"Configure logging for the script.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=getattr(logging, log_level.upper()),\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.StreamHandler(),\n",
    "            logging.FileHandler('/procedure/pnet_prostate_paper/results/minimal_pnet_training.log')\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d71fd",
   "metadata": {
    "name": "28e93629-573c-456e-8298-1d7dcbe96770"
   },
   "outputs": [],
   "source": [
    "def set_random_seeds(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    logging.info(f'Random seeds set to {seed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db3c6ce",
   "metadata": {
    "name": "f3f46640-0d7f-4175-9c69-69091da6de68"
   },
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    \"\"\"Load configuration from YAML file.\"\"\"\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        logging.info(f'Configuration loaded from {config_path}')\n",
    "        return config\n",
    "    except Exception as e:\n",
    "        logging.error(f'Failed to load configuration: {e}')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f282788",
   "metadata": {
    "name": "6e6c2e82-ef15-4625-9e33-0febdd607079"
   },
   "outputs": [],
   "source": [
    "def prepare_data(config):\n",
    "    \"\"\"\n",
    "    Load and prepare data using the project's Data class.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary from YAML file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (x_train, x_test, y_train, y_test, info_train, info_test, columns)\n",
    "    \"\"\"\n",
    "    logging.info('Loading data using Data class...')\n",
    "    \n",
    "    # Extract data parameters from config\n",
    "    data_params = config['data_params'].copy()\n",
    "    \n",
    "    # Initialize the Data class\n",
    "    data = Data(**data_params)\n",
    "    \n",
    "    # Get the data splits (train+validation combined vs test)\n",
    "    x_train, x_test, y_train, y_test, info_train, info_test, columns = data.get_train_test()\n",
    "    \n",
    "    logging.info(f'Data loaded: Train shape {x_train.shape}, Test shape {x_test.shape}')\n",
    "    \n",
    "    # Ensure y is 1D array for binary classification\n",
    "    if len(y_train.shape) > 1:\n",
    "        y_train = y_train.ravel()\n",
    "    if len(y_test.shape) > 1:\n",
    "        y_test = y_test.ravel()\n",
    "        \n",
    "    # Convert to integers for bincount\n",
    "    y_train_int = y_train.astype(int)\n",
    "    y_test_int = y_test.astype(int)\n",
    "    \n",
    "    logging.info(f'Target distribution - Train: {np.bincount(y_train_int)}, Test: {np.bincount(y_test_int)}')\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, info_train, info_test, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8847e4e6",
   "metadata": {
    "name": "f2663418-4127-43a4-83c4-63550f004edf"
   },
   "outputs": [],
   "source": [
    "def create_model(config, data_params):\n",
    "    \"\"\"\n",
    "    Create and compile P-NET model using build_pnet2.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "        data_params: Data parameters for model building\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model, feature_names)\n",
    "    \"\"\"\n",
    "    logging.info('Building P-NET model...')\n",
    "    \n",
    "    model_params = config['model_params']\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = Adam(learning_rate=model_params['learning_rate'])\n",
    "    \n",
    "    # Monkey-patch the f1 function in layers_custom to use our TF2-compatible version\n",
    "    import model.layers_custom\n",
    "    model.layers_custom.f1 = f1_score\n",
    "    \n",
    "    # Build model using build_pnet2\n",
    "    model, feature_names = build_pnet2(\n",
    "        optimizer=optimizer,\n",
    "        w_reg=model_params['w_reg'],\n",
    "        w_reg_outcomes=model_params['w_reg_outcomes'],\n",
    "        add_unk_genes=model_params['add_unk_genes'],\n",
    "        sparse=model_params['sparse'],\n",
    "        loss_weights=model_params['loss_weights'],\n",
    "        dropout=model_params['dropout'],\n",
    "        use_bias=model_params['use_bias'],\n",
    "        activation=model_params['activation'],\n",
    "        loss=model_params['loss'],\n",
    "        data_params=data_params,\n",
    "        n_hidden_layers=model_params['n_hidden_layers'],\n",
    "        direction=model_params['direction'],\n",
    "        batch_normal=model_params['batch_normal'],\n",
    "        kernel_initializer=model_params['kernel_initializer'],\n",
    "        shuffle_genes=model_params['shuffle_genes'],\n",
    "        attention=model_params['attention'],\n",
    "        dropout_testing=model_params['dropout_testing'],\n",
    "        non_neg=model_params['non_neg'],\n",
    "        repeated_outcomes=model_params['repeated_outcomes'],\n",
    "        sparse_first_layer=model_params['sparse_first_layer'],\n",
    "        ignore_missing_histology=model_params['ignore_missing_histology']\n",
    "    )\n",
    "    \n",
    "    logging.info(f'Model created with {model.count_params()} parameters')\n",
    "    return model, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32bcf47",
   "metadata": {
    "name": "8b3d86b2-53de-4674-9836-824908ad07de"
   },
   "outputs": [],
   "source": [
    "def setup_callbacks(config):\n",
    "    \"\"\"Setup training callbacks.\"\"\"\n",
    "    training_params = config['training_params']\n",
    "    callbacks = []\n",
    "    \n",
    "    # Early stopping\n",
    "    if training_params['early_stopping']:\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=training_params['patience'],\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks.append(early_stopping)\n",
    "        logging.info('Early stopping callback added')\n",
    "    \n",
    "    # Model checkpointing\n",
    "    if training_params['save_checkpoints']:\n",
    "        checkpoint_path = os.path.join(\n",
    "            training_params['checkpoint_dir'], \n",
    "            'best_model.weights.h5'  # Use weights format to avoid serialization issues\n",
    "        )\n",
    "        # Ensure checkpoint directory exists\n",
    "        os.makedirs(training_params['checkpoint_dir'], exist_ok=True)\n",
    "        \n",
    "        checkpoint = ModelCheckpoint(\n",
    "            checkpoint_path,\n",
    "            monitor='val_loss',\n",
    "            save_best_only=training_params['save_best_only'],\n",
    "            save_weights_only=True,  # Save only weights to avoid serialization issues\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks.append(checkpoint)\n",
    "        logging.info(f'Model checkpoint callback added: {checkpoint_path}')\n",
    "    \n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db19a5ee",
   "metadata": {
    "name": "53139934-158b-4163-80f2-0906e2720b59"
   },
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_test, y_test, config):\n",
    "    \"\"\"\n",
    "    Train the P-NET model.\n",
    "    \n",
    "    Args:\n",
    "        model: Compiled Keras model\n",
    "        x_train, y_train: Training data\n",
    "        x_test, y_test: Test data for validation\n",
    "        config: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        History object from model training\n",
    "    \"\"\"\n",
    "    logging.info('Starting model training...')\n",
    "    \n",
    "    model_params = config['model_params']\n",
    "    training_params = config['training_params']\n",
    "    \n",
    "    # Setup callbacks\n",
    "    callbacks = setup_callbacks(config)\n",
    "    \n",
    "    # Prepare y data for multi-output model if needed\n",
    "    if hasattr(model, 'output_names') and len(model.output_names) > 1:\n",
    "        # For multi-output models, replicate y for each output\n",
    "        y_train_multi = [y_train] * len(model.output_names)\n",
    "        y_test_multi = [y_test] * len(model.output_names)\n",
    "    else:\n",
    "        y_train_multi = y_train\n",
    "        y_test_multi = y_test\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x_train, y_train_multi,\n",
    "        batch_size=model_params['batch_size'],\n",
    "        epochs=model_params['epochs'],\n",
    "        validation_data=(x_test, y_test_multi),\n",
    "        callbacks=callbacks,\n",
    "        verbose=training_params['verbose']\n",
    "    )\n",
    "    \n",
    "    logging.info('Model training completed')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d30ee3",
   "metadata": {
    "name": "9eee0bd3-596b-44fd-9509-cdd4ad235145"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_test, y_test, config):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model and generate ROC curve.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        x_test, y_test: Test data\n",
    "        config: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    logging.info('Evaluating model...')\n",
    "    \n",
    "    # Prepare y data for multi-output model if needed\n",
    "    if hasattr(model, 'output_names') and len(model.output_names) > 1:\n",
    "        y_test_multi = [y_test] * len(model.output_names)\n",
    "    else:\n",
    "        y_test_multi = y_test\n",
    "    \n",
    "    # Evaluate model\n",
    "    test_results = model.evaluate(x_test, y_test_multi, verbose=0)\n",
    "    \n",
    "    # Extract loss from results\n",
    "    if isinstance(test_results, list):\n",
    "        test_loss = test_results[0]  # First value is always the total loss\n",
    "    else:\n",
    "        test_loss = test_results\n",
    "        \n",
    "    logging.info(f'Test loss: {test_loss:.4f}')\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict(x_test, verbose=0)\n",
    "    \n",
    "    # Handle multiple outputs (P-NET can have multiple outputs)\n",
    "    if isinstance(y_pred_proba, list):\n",
    "        # Use the last output (final decision layer)\n",
    "        y_pred_proba = y_pred_proba[-1]\n",
    "    \n",
    "    # Ensure correct shape\n",
    "    if y_pred_proba.ndim > 1:\n",
    "        y_pred_proba = y_pred_proba.flatten()\n",
    "    \n",
    "    # Calculate ROC AUC\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    logging.info(f'ROC AUC: {roc_auc:.4f}')\n",
    "    \n",
    "    # Generate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve - P-NET Model')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Remove top and right spines for cleaner look\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    # Save plot\n",
    "    plot_path = config['training_params']['plot_save_path']\n",
    "    os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    logging.info(f'ROC curve saved to {plot_path}')\n",
    "    \n",
    "    # Compile evaluation metrics\n",
    "    eval_metrics = {\n",
    "        'test_loss': test_loss,\n",
    "        'roc_auc': roc_auc,\n",
    "        'test_accuracy': test_results[1] if isinstance(test_results, list) and len(test_results) > 1 else 0.0\n",
    "    }\n",
    "    \n",
    "    return eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7012d92",
   "metadata": {
    "name": "ad2dbfaf-77c5-4590-9f42-72f8a6c0fb11"
   },
   "outputs": [],
   "source": [
    "def save_results(model, eval_metrics, config, feature_names=None):\n",
    "    \"\"\"\n",
    "    Save training results and model artifacts.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        eval_metrics: Evaluation metrics dictionary\n",
    "        config: Configuration dictionary\n",
    "        feature_names: Feature names from model building\n",
    "    \"\"\"\n",
    "    results_dir = config['training_params']['results_dir']\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Save evaluation metrics\n",
    "    metrics_file = os.path.join(results_dir, 'evaluation_metrics.yaml')\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        yaml.dump(eval_metrics, f, default_flow_style=False)\n",
    "    logging.info(f'Evaluation metrics saved to {metrics_file}')\n",
    "    \n",
    "    # Save model summary\n",
    "    summary_file = os.path.join(results_dir, 'model_summary.txt')\n",
    "    with open(summary_file, 'w') as f:\n",
    "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    logging.info(f'Model summary saved to {summary_file}')\n",
    "    \n",
    "    # Save feature names if available\n",
    "    if feature_names is not None:\n",
    "        feature_file = os.path.join(results_dir, 'feature_names.yaml')\n",
    "        with open(feature_file, 'w') as f:\n",
    "            yaml.dump(feature_names, f, default_flow_style=False)\n",
    "        logging.info(f'Feature names saved to {feature_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d8940c",
   "metadata": {
    "name": "b990ef6d-cd28-4339-af10-312ab84ae3c7"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Train P-NET model on minimal prostate dataset'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--config',\n",
    "        type=str,\n",
    "        default='/procedure/pnet_prostate_paper/config/minimal_training_params.yml',\n",
    "        help='Path to configuration YAML file'\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load configuration\n",
    "    config = load_config(args.config)\n",
    "    \n",
    "    # Setup logging\n",
    "    logger = setup_logging(config['training_params']['log_level'])\n",
    "    logger.info('Starting P-NET minimal training script')\n",
    "    logger.info(f'Configuration: {args.config}')\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    set_random_seeds(config['training_params']['random_seed'])\n",
    "    \n",
    "    # Prepare data\n",
    "    x_train, x_test, y_train, y_test, info_train, info_test, columns = prepare_data(config)\n",
    "    \n",
    "    # Create model\n",
    "    model, feature_names = create_model(config, config['data_params'])\n",
    "    \n",
    "    # Train model\n",
    "    history = train_model(model, x_train, y_train, x_test, y_test, config)\n",
    "    \n",
    "    # Evaluate model\n",
    "    eval_metrics = evaluate_model(model, x_test, y_test, config)\n",
    "    \n",
    "    # Save results\n",
    "    save_results(model, eval_metrics, config, feature_names)\n",
    "    \n",
    "    # Print final results\n",
    "    logger.info('Training completed successfully!')\n",
    "    logger.info(f'Final Results:')\n",
    "    for metric, value in eval_metrics.items():\n",
    "        logger.info(f'  {metric}: {value:.4f}')\n",
    "    \n",
    "    return model, eval_metrics, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4308929d",
   "metadata": {
    "name": "5749c745-dddf-4c47-8697-46803006accc"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    model, eval_metrics, feature_names = main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
